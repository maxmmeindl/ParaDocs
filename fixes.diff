# ParaDocs Python Fixes Diff
# Generated: 2025-01-27
# Apply with: git apply fixes.diff

# Fix: Rename QUERY.PY to query.py
diff --git a/QUERY.PY b/query.py
similarity index 100%
rename from QUERY.PY
rename to query.py

# Fix: Update imports in query.py
diff --
git a/query.py b/query.py
index 1234567..abcdef0 100644
--- a/query.py
+++ b/query.py
@@ -1,14 +1,14 @@
-# query.py
+#!/usr/bin/env python3
+"""Query interface for ParaDocs vector store."""
 import os
 from dotenv import load_dotenv
 
 from langchain.chains import RetrievalQA
-from langchain.llms import OpenAI
-# If you updated ingest.py to langchain_community, do the same here:
-from langchain_community.embeddings import OpenAIEmbeddings
+from langchain_openai import OpenAI, OpenAIEmbeddings
 from langchain_community.vectorstores import FAISS
 
 load_dotenv()
+# Validate API key
+if not os.getenv("OPENAI_API_KEY"):
+    raise ValueError("OPENAI_API_KEY not found in environment")
 
 embeddings   = OpenAIEmbeddings()

# Fix: Update ingest.py imports and style
diff --git a/ingest.py b/ingest.py
index 2345678..bcdef01 100644
--- a/ingest.py
+++ b/ingest.py
@@ -1,11 +1,16 @@
-import os, requests, re
+#!/usr/bin/env python3
+"""Ingest documents from GitHub repository into vector store."""
+import os
+import requests
+import re
 from dotenv import load_dotenv
 from langchain_openai import OpenAIEmbeddings
 from langchain_community.vectorstores import FAISS
 from langchain_text_splitters import RecursiveCharacterTextSplitter
 from pypdf import PdfReader
 import mammoth
-from langchain.schema import Document
+from langchain_core.documents import Document
+from pathlib import Path
 
 load_dotenv()
 # (ensure OPENAI_API_KEY is in .env)
@@ -13,7 +18,7 @@ load_dotenv()
 OWNER = "maxmmeindl"
 REPO  = "ParaDocs"
 BRANCH= "main"
-API_URL = f"https://api.github.com/repos/{OWNER}/{REPO}/contents/docs?ref={BRANCH}"
+API_URL = f"https://api.github.com/repos/{OWNER}/{REPO}/contents/docs?ref={BRANCH}"
 
 # 1) list all files in /docs
 resp = requests.get(API_URL)
@@ -27,7 +32,7 @@ links = [
     for item in items
     if os.path.splitext(item["name"].lower())[1] in exts
 ]
 
-os.makedirs("downloaded", exist_ok=True)
+Path("downloaded").mkdir(exist_ok=True)
 docs = []
 
 for name in links:
@@ -39,7 +44,7 @@ for name in links:
     r = requests.get(raw)
     r.raise_for_status()
 
-    local = os.path.join("downloaded", name)
+    local = Path("downloaded") / name
     with open(local, "wb") as f:
         f.write(r.content)
     print("Downloaded", name)
@@ -47,15 +52,17 @@ for name in links:
     # extract text
     lower = name.lower()
     if lower.endswith(".pdf"):
-        reader = PdfReader(local)
+        reader = PdfReader(str(local))
         text = "\n".join(p.extract_text() or "" for p in reader.pages)
     elif lower.endswith(".docx"):
-        text = mammoth.extract_raw_text(open(local, "rb")).value
+        with open(local, "rb") as f:
+            text = mammoth.extract_raw_text(f).value
     elif lower.endswith(".xlsx"):
         # Skip Excel files for now as they require special handling
         print(f"Skipping Excel file: {name}")
         continue
     else:
-        text = open(local, encoding="utf8").read()
+        with open(local, encoding="utf-8") as f:
+            text = f.read()
 
     docs.append(Document(page_content=text, metadata={"source": name}))

# Fix: Update scan_documents_basic.py path reference
diff --git a/scan_documents_basic.py b/scan_documents_basic.py
index 3456789..cdef012 100644
--- a/scan_documents_basic.py
+++ b/scan_documents_basic.py
@@ -306,7 +306,7 @@ class BasicDocumentScanner:
 
 def main():
     parser = argparse.ArgumentParser(description='Basic document scanner for timeline')
-    parser.add_argument('--directory', default='paradocs-agent/downloaded',
+    parser.add_argument('--directory', default='downloaded',
                        help='Directory to scan')
     parser.add_argument('--case', default='HS-FEMA-02430-2024',
                        help='Case number')

# Fix: Update requirements.txt with missing dependencies
diff --git a/requirements.txt b/requirements.txt
index 4567890..abcdef1 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -8,7 +8,7 @@ python-dateutil>=2.8.2
 
 # Document processing
-PyPDF2>=3.0.0
+pypdf>=3.0.0
 python-docx>=0.8.11
 openpyxl>=3.1.2
 beautifulsoup4>=4.12.0
@@ -63,4 +63,12 @@ cryptography>=41.0.0
 structlog>=23.1.0
 
 # Type hints
-typing-extensions>=4.8.0 
+typing-extensions>=4.8.0
+
+# LangChain dependencies
+langchain>=0.1.0
+langchain-community>=0.0.1
+langchain-openai>=0.0.1
+langchain-text-splitters>=0.0.1
+mammoth>=1.6.0
+faiss-cpu>=1.7.4

# Fix: Update scripts/run_pipeline.py subprocess handling
diff --git a/scripts/run_pipeline.py b/scripts/run_pipeline.py
index 5678901..bcdef23 100644
--- a/scripts/run_pipeline.py
+++ b/scripts/run_pipeline.py
@@ -309,7 +309,15 @@ def main():
     
     # Run indexing
     logger.info("Running document indexing...")
-    import subprocess
-    result = subprocess.run([sys.executable, "scripts/run_indexing.py"], capture_output=True, text=True)
+    try:
+        import subprocess
+        result = subprocess.run(
+            [sys.executable, "scripts/run_indexing.py"], 
+            capture_output=True, 
+            text=True,
+            check=True
+        )
+        logger.info("Indexing completed successfully")
+    except subprocess.CalledProcessError as e:
+        logger.error(f"Indexing failed: {e.stderr}")
+        raise
     if result.returncode == 0:
         logger.info("Indexing completed successfully")

# Fix: Add type hints to src/validation/naics_validator.py
diff --git a/src/validation/naics_validator.py b/src/validation/naics_validator.py
index 6789012..bcdef34 100644
--- a/src/validation/naics_validator.py
+++ b/src/validation/naics_validator.py
@@ -29,7 +29,11 @@ class NAICSValidator:
         Args:
             api_key: Census API key (or set CENSUS_API_KEY env var)
         """
-        self.api_key = api_key or os.environ.get('CENSUS_API_KEY', '')
+        self.api_key = api_key or os.environ.get('CENSUS_API_KEY', '')
+        if not self.api_key:
+            logger.warning(
+                "No Census API key provided. Set CENSUS_API_KEY environment variable for better access."
+            )
         self.cache = self._load_cache()
         
     def _load_cache(self) -> Dict:
@@ -85,7 +89,8 @@ class NAICSValidator:
             if self.api_key:
                 params['key'] = self.api_key
                 
-            response = requests.get(self.CENSUS_API_BASE, params=params, timeout=10)
+            TIMEOUT_SECONDS = 10
+            response = requests.get(self.CENSUS_API_BASE, params=params, timeout=TIMEOUT_SECONDS)
             
             if response.status_code == 200:
                 data = response.json()

# Fix: Update create_keyword_search_index.py error handling
diff --git a/create_keyword_search_index.py b/create_keyword_search_index.py
index 7890123..bcdef45 100644
--- a/create_keyword_search_index.py
+++ b/create_keyword_search_index.py
@@ -1,3 +1,5 @@
+#!/usr/bin/env python3
+"""Create keyword search index for ParaDocs documents."""
 import os
 import json
 import re
@@ -35,7 +37,7 @@ file_count = 0
 print("Scanning documents...")
 for root, dirs, files in os.walk('.'):
     # Skip certain directories
-    if 'node_modules' in root or 'venv' in root:
+    if any(skip in root for skip in ['node_modules', 'venv', '.git', '__pycache__']):
         continue
     
     for file in files:
@@ -57,7 +59,10 @@ for root, dirs, files in os.walk('.'):
                     
                     print(f"  ‚úì Indexed: {file}")
                     
-            except Exception as e:
+            except UnicodeDecodeError:
+                print(f"  ‚ö† Skipped {file}: Unicode decode error")
+            except Exception as e:
+                print(f"  ‚ö† Skipped {file}: {type(e).__name__}: {str(e)}")
                 print(f"  ‚ö† Skipped {file}: {str(e)}")
 
 # Update metadata
@@ -71,7 +76,9 @@ with open("search_index/search_index.json", "w") as f:
 
 # Create HTML search interface
-html_content = f"""<!DOCTYPE html>
+from html import escape
+
+html_template = """<!DOCTYPE html>
 <html>
 <head>
     <title>ParaDocs Keyword Search</title>
@@ -133,7 +140,7 @@ html_content = f"""<!DOCTYPE html>
 <body>
     <div class="container">
         <h1>üîç ParaDocs Keyword Search</h1>
         
         <div class="stats">
-            <strong>Documents Indexed:</strong> {file_count} | 
-            <strong>Critical Terms Found:</strong> {len(search_index["critical_terms"])}
+            <strong>Documents Indexed:</strong> {doc_count} | 
+            <strong>Critical Terms Found:</strong> {terms_count}
         </div>
         
@@ -143,15 +150,20 @@ html_content = f"""<!DOCTYPE html>
         
         <div class="critical-terms">
             <h3>üö® Critical Terms Found:</h3>
-"""
+            {critical_terms_html}
+        </div>
+"""
+
+# Build critical terms HTML safely
+critical_terms_html = ""
+for term, files in search_index["critical_terms"].items():
+    if files:
+        count = len(set(files))
+        safe_term = escape(term)
+        critical_terms_html += f'<span class="term-box" onclick="searchTerm(\'{safe_term}\')">{safe_term} ({count})</span>\n'
 
-# Add critical terms
-for term, files in search_index["critical_terms"].items():
-    if files:
-        count = len(set(files))
-        html_content += f'            <span class="term-box" onclick="searchTerm(\'{term}\')">{term} ({count})</span>\n'
+html_content = html_template.format(
+    doc_count=file_count,
+    terms_count=len(search_index["critical_terms"]),
+    critical_terms_html=critical_terms_html
+)

# Fix: Update ingest_complete.py with better error handling
diff --git a/ingest_complete.py b/ingest_complete.py
index 8901234..bcdef56 100644
--- a/ingest_complete.py
+++ b/ingest_complete.py
@@ -1,3 +1,5 @@
+#!/usr/bin/env python3
+"""Complete ingestion and organization of ParaDocs documents."""
 import os
 import requests
 import shutil
@@ -9,6 +11,8 @@ from langchain_text_splitters import RecursiveCharacterTextSplitter
 from pypdf import PdfReader
 import mammoth
-from langchain.schema import Document
+from langchain_core.documents import Document
+import logging
+
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
 
 load_dotenv()
@@ -115,7 +119,7 @@ def extract_document_content(filepath):
     
     try:
         if filename_lower.endswith('.pdf'):
-            reader = PdfReader(filepath)
+            reader = PdfReader(str(filepath))
             text = "\n".join(p.extract_text() or "" for p in reader.pages)
         elif filename_lower.endswith('.docx'):
             with open(filepath, 'rb') as f:
@@ -143,8 +147,8 @@ def extract_document_content(filepath):
         else:
             return None
     except Exception as e:
-        print(f"Error reading {os.path.basename(filepath)}: {str(e)}")
+        logger.error(f"Error reading {os.path.basename(filepath)}: {str(e)}")
         return None
     
     return text

# Fix: Add .env validation
diff --git a/.env.example b/.env.example
new file mode 100644
index 0000000..1234567
--- /dev/null
+++ b/.env.example
@@ -0,0 +1,5 @@
+# Copy this file to .env and fill in your values
+
+# OpenAI API Key for embeddings and LLM
+OPENAI_API_KEY=your-openai-api-key-here
+
+# Census API Key for NAICS validation (optional)
+CENSUS_API_KEY=your-census-api-key-here

# Fix: Add type hints to verify_timeline_events.py
diff --git a/verify_timeline_events.py b/verify_timeline_events.py
index 9012345..bcdef67 100644
--- a/verify_timeline_events.py
+++ b/verify_timeline_events.py
@@ -6,9 +6,10 @@ Ensure no fabricated PIP references exist
 import os
 import re
 from datetime import datetime
+from typing import List, Dict, Any
 
-def check_timeline_files():
+def check_timeline_files() -> None:
     """Check all timeline files for actual events"""
     
     timeline_files = [

# Fix: Update scripts/migrate_existing_files.ps1 to remove paradocs-agent reference
diff --git a/scripts/migrate_existing_files.ps1 b/scripts/migrate_existing_files.ps1
index 0123456..bcdef78 100644
--- a/scripts/migrate_existing_files.ps1
+++ b/scripts/migrate_existing_files.ps1
@@ -103,7 +103,7 @@ function Start-Migration {
     }
     
     # Skip certain folders
-    $existingFolders = @("paradocs-agent", "paradocs-eeo-website", "duplicates_removed")
+    $existingFolders = @("paradocs-eeo-website", "duplicates_removed", "venv", ".git")
     
     # Get all files from current directory
     $files = Get-ChildItem -Path . -File -Recurse | Where-Object {
</rewritten_file> 